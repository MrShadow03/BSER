{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.core.fromnumeric import shape\n",
    "import librosa\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os\n",
    "# import pywt\n",
    "import csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import normalize, to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras,nn\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def model2d(input_shape, num_classes):\n",
    "\n",
    "    model = keras.Sequential(name='model2d')\n",
    "\n",
    "    #LFLB1\n",
    "    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1,padding='same',input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    #LFLB2\n",
    "    model.add(layers.Conv2D(filters=64,kernel_size=3,strides=1, padding='same', ))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
    "    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "    #LFLB3\n",
    "    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
    "    #model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "    \n",
    "    #LFLB4\n",
    "    model.add(layers.Conv2D(filters=128,kernel_size=3,strides=1,padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=4, strides=4))\n",
    "\n",
    "    #model.add(layers.Reshape((-1, 128)))\n",
    "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "    \n",
    "    #LSTM\n",
    "    #model.add(layers.LSTM(256))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(256)))\n",
    "    \n",
    "    #model.add(keras.layers.Dense(128,activation=nn.relu))\n",
    "    #model.add(layers.Dense(128,activation=nn.relu))\n",
    "    model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0006, decay=1e-6)\n",
    "\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Clear TensorFlow session to release resources\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Ensure memory growth is set\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "else:\n",
    "    print(\"No GPU found. Training will proceed on the CPU.\")\n",
    "\n",
    "def train(train_data_x, train_data_y, emotion, emotionNumber, epochs=10):\n",
    "    model = model2d(input_shape=(128, 257, 3), num_classes=emotionNumber)\n",
    "    model.summary()\n",
    "    \n",
    "    batch_size = 5\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=20)\n",
    "    \n",
    "    mc = ModelCheckpoint(path+'/model/'+emotion+'_max_model.keras', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    history = model.fit(train_data_x, train_data_y, validation_data=(test_data_x, test_data_y), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[es, mc])\n",
    "    \n",
    "    # Save the train and test accuracy/loss at each epoch to a CSV file\n",
    "    with open(path + '/model/' + emotion + '_training_history.csv', mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train Accuracy', 'Train Loss', 'Test Accuracy', 'Test Loss'])\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_acc = history.history['categorical_accuracy'][epoch] if epoch < len(history.history['categorical_accuracy']) else None\n",
    "            train_loss = history.history['loss'][epoch] if epoch < len(history.history['loss']) else None\n",
    "            val_acc = history.history['val_categorical_accuracy'][epoch] if epoch < len(history.history['val_categorical_accuracy']) else None\n",
    "            val_loss = history.history['val_loss'][epoch] if epoch < len(history.history['val_loss']) else None\n",
    "            \n",
    "            writer.writerow([epoch + 1, train_acc, train_loss, val_acc, val_loss])\n",
    "    \n",
    "    acc = history.history['categorical_accuracy'][-1]\n",
    "    \n",
    "    model.save(path+'/model/'+emotion+'_max_model.keras')\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def test(test_data_x, test_data_y, emotion):\n",
    "    \n",
    "    new_model = load_model(path+'/model/'+emotion+'_max_model.keras')\n",
    "    history=new_model.evaluate(test_data_x, test_data_y, batch_size=1)\n",
    "    predict=new_model.predict(test_data_x)\n",
    "    return history[1]\n",
    "\n",
    "def maxIndex(data):\n",
    "  max=data[0]\n",
    "  index=0\n",
    "  for i in range(1,len(data)):\n",
    "    if(max<data[i]):\n",
    "      max=data[i]\n",
    "      index=i\n",
    "  return index  \n",
    "\n",
    "def test_emotion(test_data_x, test_data_y, total, emotion, path):\n",
    "    model_path = os.path.join(path, 'model', emotion + '_max_model.keras')\n",
    "    \n",
    "    # Check if the file exists before loading\n",
    "    if not os.path.isfile(model_path):\n",
    "        raise ValueError(f\"File not found: {model_path}. Please ensure the file is an accessible `.keras` zip file.\")\n",
    "    \n",
    "    new_model = load_model(model_path)\n",
    "    \n",
    "    # Ensure the labels are one-hot encoded for testing\n",
    "    test_data_y = to_categorical(test_data_y, num_classes=total)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    history = new_model.evaluate(test_data_x, test_data_y, batch_size=5)\n",
    "    print(f\"Test accuracy: {history[1]}\")\n",
    "    \n",
    "    # Predict and generate confusion matrix\n",
    "    predictions = new_model.predict(test_data_x, batch_size=5)\n",
    "    confusion_matrix = np.zeros((total, total))\n",
    "    \n",
    "    for i in range(len(test_data_y)):\n",
    "        actual_label = np.argmax(test_data_y[i])\n",
    "        predicted_label = np.argmax(predictions[i])\n",
    "        confusion_matrix[actual_label][predicted_label] += 1\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    save_dir = os.path.join(path, 'data')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the confusion matrix to CSV\n",
    "    confusion_csv_path = os.path.join(save_dir, emotion + '_confusion.csv')\n",
    "    with open(confusion_csv_path, \"w\", newline='') as my_csv:\n",
    "        csvWriter = csv.writer(my_csv, delimiter=',')\n",
    "        csvWriter.writerows(confusion_matrix)\n",
    "    \n",
    "    print(f\"Confusion matrix saved to: {confusion_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 128, 257, 3)\n",
      "(4000,)\n",
      "(1000, 128, 257, 3)\n",
      "(1000,)\n",
      "Model: \"model2d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 128, 257, 64)      1792      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 128, 257, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128, 257, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 64, 128, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 128, 64)       36928     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64, 128, 64)      256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64, 128, 64)       0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 16, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 16, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 16, 32, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 1, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 1, 256)           0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 512)              1050624   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,314,885\n",
      "Trainable params: 1,314,117\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Model: \"model2d\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 128, 257, 64)      1792      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 128, 257, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128, 257, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 64, 128, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 128, 64)       36928     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64, 128, 64)      256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64, 128, 64)       0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 16, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 16, 32, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 16, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 16, 32, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 4, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 4, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 1, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 1, 256)           0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 512)              1050624   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,314,885\n",
      "Trainable params: 1,314,117\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.58600, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 76s - loss: 1.1576 - categorical_accuracy: 0.5020 - val_loss: 0.9608 - val_categorical_accuracy: 0.5860 - 76s/epoch - 95ms/step\n",
      "Epoch 2/60\n",
      "\n",
      "Epoch 2: val_categorical_accuracy improved from 0.58600 to 0.71400, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 23s - loss: 0.8689 - categorical_accuracy: 0.6300 - val_loss: 0.7006 - val_categorical_accuracy: 0.7140 - 23s/epoch - 29ms/step\n",
      "Epoch 3/60\n",
      "\n",
      "Epoch 3: val_categorical_accuracy improved from 0.71400 to 0.76500, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 15s - loss: 0.7153 - categorical_accuracy: 0.7103 - val_loss: 0.6216 - val_categorical_accuracy: 0.7650 - 15s/epoch - 18ms/step\n",
      "Epoch 4/60\n",
      "\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.76500\n",
      "800/800 - 14s - loss: 0.5589 - categorical_accuracy: 0.7820 - val_loss: 0.7787 - val_categorical_accuracy: 0.6960 - 14s/epoch - 18ms/step\n",
      "Epoch 5/60\n",
      "\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.76500\n",
      "800/800 - 14s - loss: 0.4515 - categorical_accuracy: 0.8257 - val_loss: 0.8487 - val_categorical_accuracy: 0.6930 - 14s/epoch - 17ms/step\n",
      "Epoch 6/60\n",
      "\n",
      "Epoch 6: val_categorical_accuracy improved from 0.76500 to 0.83900, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.3350 - categorical_accuracy: 0.8733 - val_loss: 0.4352 - val_categorical_accuracy: 0.8390 - 14s/epoch - 18ms/step\n",
      "Epoch 7/60\n",
      "\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.83900\n",
      "800/800 - 14s - loss: 0.2637 - categorical_accuracy: 0.8997 - val_loss: 0.4991 - val_categorical_accuracy: 0.8090 - 14s/epoch - 17ms/step\n",
      "Epoch 8/60\n",
      "\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.83900\n",
      "800/800 - 14s - loss: 0.2089 - categorical_accuracy: 0.9205 - val_loss: 0.8045 - val_categorical_accuracy: 0.7790 - 14s/epoch - 17ms/step\n",
      "Epoch 9/60\n",
      "\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.83900\n",
      "800/800 - 14s - loss: 0.1510 - categorical_accuracy: 0.9465 - val_loss: 0.5158 - val_categorical_accuracy: 0.8190 - 14s/epoch - 17ms/step\n",
      "Epoch 10/60\n",
      "\n",
      "Epoch 10: val_categorical_accuracy improved from 0.83900 to 0.87700, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.1458 - categorical_accuracy: 0.9417 - val_loss: 0.3929 - val_categorical_accuracy: 0.8770 - 14s/epoch - 18ms/step\n",
      "Epoch 11/60\n",
      "\n",
      "Epoch 11: val_categorical_accuracy improved from 0.87700 to 0.89800, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.1122 - categorical_accuracy: 0.9585 - val_loss: 0.3102 - val_categorical_accuracy: 0.8980 - 14s/epoch - 18ms/step\n",
      "Epoch 12/60\n",
      "\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.89800\n",
      "800/800 - 14s - loss: 0.0990 - categorical_accuracy: 0.9632 - val_loss: 0.3440 - val_categorical_accuracy: 0.8950 - 14s/epoch - 17ms/step\n",
      "Epoch 13/60\n",
      "\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.89800\n",
      "800/800 - 14s - loss: 0.0903 - categorical_accuracy: 0.9665 - val_loss: 0.3519 - val_categorical_accuracy: 0.8830 - 14s/epoch - 18ms/step\n",
      "Epoch 14/60\n",
      "\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.89800\n",
      "800/800 - 14s - loss: 0.0703 - categorical_accuracy: 0.9753 - val_loss: 0.4698 - val_categorical_accuracy: 0.8670 - 14s/epoch - 17ms/step\n",
      "Epoch 15/60\n",
      "\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.89800\n",
      "800/800 - 14s - loss: 0.0679 - categorical_accuracy: 0.9770 - val_loss: 0.6596 - val_categorical_accuracy: 0.8360 - 14s/epoch - 17ms/step\n",
      "Epoch 16/60\n",
      "\n",
      "Epoch 16: val_categorical_accuracy improved from 0.89800 to 0.90500, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.0707 - categorical_accuracy: 0.9745 - val_loss: 0.3243 - val_categorical_accuracy: 0.9050 - 14s/epoch - 18ms/step\n",
      "Epoch 17/60\n",
      "\n",
      "Epoch 17: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0555 - categorical_accuracy: 0.9793 - val_loss: 0.3773 - val_categorical_accuracy: 0.8930 - 14s/epoch - 17ms/step\n",
      "Epoch 18/60\n",
      "\n",
      "Epoch 18: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0492 - categorical_accuracy: 0.9805 - val_loss: 0.4067 - val_categorical_accuracy: 0.8930 - 14s/epoch - 17ms/step\n",
      "Epoch 19/60\n",
      "\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0542 - categorical_accuracy: 0.9793 - val_loss: 0.5636 - val_categorical_accuracy: 0.8440 - 14s/epoch - 17ms/step\n",
      "Epoch 20/60\n",
      "\n",
      "Epoch 20: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0407 - categorical_accuracy: 0.9862 - val_loss: 0.6944 - val_categorical_accuracy: 0.8490 - 14s/epoch - 17ms/step\n",
      "Epoch 21/60\n",
      "\n",
      "Epoch 21: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0324 - categorical_accuracy: 0.9883 - val_loss: 0.7827 - val_categorical_accuracy: 0.8170 - 14s/epoch - 17ms/step\n",
      "Epoch 22/60\n",
      "\n",
      "Epoch 22: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0457 - categorical_accuracy: 0.9858 - val_loss: 0.3855 - val_categorical_accuracy: 0.8890 - 14s/epoch - 17ms/step\n",
      "Epoch 23/60\n",
      "\n",
      "Epoch 23: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0464 - categorical_accuracy: 0.9835 - val_loss: 0.4656 - val_categorical_accuracy: 0.8700 - 14s/epoch - 17ms/step\n",
      "Epoch 24/60\n",
      "\n",
      "Epoch 24: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0343 - categorical_accuracy: 0.9890 - val_loss: 1.0025 - val_categorical_accuracy: 0.7930 - 14s/epoch - 17ms/step\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 25: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0358 - categorical_accuracy: 0.9872 - val_loss: 0.6027 - val_categorical_accuracy: 0.8450 - 14s/epoch - 17ms/step\n",
      "Epoch 26/60\n",
      "\n",
      "Epoch 26: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0453 - categorical_accuracy: 0.9837 - val_loss: 0.4678 - val_categorical_accuracy: 0.8820 - 14s/epoch - 17ms/step\n",
      "Epoch 27/60\n",
      "\n",
      "Epoch 27: val_categorical_accuracy did not improve from 0.90500\n",
      "800/800 - 14s - loss: 0.0285 - categorical_accuracy: 0.9900 - val_loss: 0.3853 - val_categorical_accuracy: 0.8950 - 14s/epoch - 17ms/step\n",
      "Epoch 28/60\n",
      "\n",
      "Epoch 28: val_categorical_accuracy improved from 0.90500 to 0.91300, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.0272 - categorical_accuracy: 0.9910 - val_loss: 0.3280 - val_categorical_accuracy: 0.9130 - 14s/epoch - 18ms/step\n",
      "Epoch 29/60\n",
      "\n",
      "Epoch 29: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0235 - categorical_accuracy: 0.9910 - val_loss: 0.3976 - val_categorical_accuracy: 0.8890 - 14s/epoch - 17ms/step\n",
      "Epoch 30/60\n",
      "\n",
      "Epoch 30: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0217 - categorical_accuracy: 0.9930 - val_loss: 0.3842 - val_categorical_accuracy: 0.8930 - 14s/epoch - 17ms/step\n",
      "Epoch 31/60\n",
      "\n",
      "Epoch 31: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0425 - categorical_accuracy: 0.9830 - val_loss: 0.6566 - val_categorical_accuracy: 0.8370 - 14s/epoch - 17ms/step\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 32: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0287 - categorical_accuracy: 0.9908 - val_loss: 0.5627 - val_categorical_accuracy: 0.8690 - 14s/epoch - 17ms/step\n",
      "Epoch 33/60\n",
      "\n",
      "Epoch 33: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0205 - categorical_accuracy: 0.9937 - val_loss: 0.3651 - val_categorical_accuracy: 0.9110 - 14s/epoch - 17ms/step\n",
      "Epoch 34/60\n",
      "\n",
      "Epoch 34: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0241 - categorical_accuracy: 0.9902 - val_loss: 0.4818 - val_categorical_accuracy: 0.8850 - 14s/epoch - 17ms/step\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 35: val_categorical_accuracy did not improve from 0.91300\n",
      "800/800 - 14s - loss: 0.0290 - categorical_accuracy: 0.9905 - val_loss: 0.7815 - val_categorical_accuracy: 0.8150 - 14s/epoch - 17ms/step\n",
      "Epoch 36/60\n",
      "\n",
      "Epoch 36: val_categorical_accuracy improved from 0.91300 to 0.91400, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.0282 - categorical_accuracy: 0.9887 - val_loss: 0.3526 - val_categorical_accuracy: 0.9140 - 14s/epoch - 18ms/step\n",
      "Epoch 37/60\n",
      "\n",
      "Epoch 37: val_categorical_accuracy did not improve from 0.91400\n",
      "800/800 - 14s - loss: 0.0286 - categorical_accuracy: 0.9905 - val_loss: 0.5017 - val_categorical_accuracy: 0.8760 - 14s/epoch - 17ms/step\n",
      "Epoch 38/60\n",
      "\n",
      "Epoch 38: val_categorical_accuracy did not improve from 0.91400\n",
      "800/800 - 14s - loss: 0.0058 - categorical_accuracy: 0.9983 - val_loss: 0.6022 - val_categorical_accuracy: 0.8520 - 14s/epoch - 17ms/step\n",
      "Epoch 39/60\n",
      "\n",
      "Epoch 39: val_categorical_accuracy did not improve from 0.91400\n",
      "800/800 - 14s - loss: 0.0410 - categorical_accuracy: 0.9858 - val_loss: 0.4489 - val_categorical_accuracy: 0.9030 - 14s/epoch - 17ms/step\n",
      "Epoch 40/60\n",
      "\n",
      "Epoch 40: val_categorical_accuracy improved from 0.91400 to 0.92500, saving model to ./data/SUBESCOxUIU/train/SUBESCO/model\\SUBESCO_latest_max_model.keras\n",
      "800/800 - 14s - loss: 0.0129 - categorical_accuracy: 0.9965 - val_loss: 0.3248 - val_categorical_accuracy: 0.9250 - 14s/epoch - 18ms/step\n",
      "Epoch 41/60\n",
      "\n",
      "Epoch 41: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0366 - categorical_accuracy: 0.9855 - val_loss: 0.3030 - val_categorical_accuracy: 0.9250 - 14s/epoch - 17ms/step\n",
      "Epoch 42/60\n",
      "\n",
      "Epoch 42: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0029 - categorical_accuracy: 0.9992 - val_loss: 0.3245 - val_categorical_accuracy: 0.9240 - 14s/epoch - 17ms/step\n",
      "Epoch 43/60\n",
      "\n",
      "Epoch 43: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 2.7067e-04 - categorical_accuracy: 1.0000 - val_loss: 0.3261 - val_categorical_accuracy: 0.9210 - 14s/epoch - 17ms/step\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 44: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0503 - categorical_accuracy: 0.9820 - val_loss: 0.3225 - val_categorical_accuracy: 0.9230 - 14s/epoch - 17ms/step\n",
      "Epoch 45/60\n",
      "\n",
      "Epoch 45: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0236 - categorical_accuracy: 0.9923 - val_loss: 0.4151 - val_categorical_accuracy: 0.8970 - 14s/epoch - 17ms/step\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 46: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0075 - categorical_accuracy: 0.9973 - val_loss: 0.3550 - val_categorical_accuracy: 0.9220 - 14s/epoch - 17ms/step\n",
      "Epoch 47/60\n",
      "\n",
      "Epoch 47: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0360 - categorical_accuracy: 0.9872 - val_loss: 0.3872 - val_categorical_accuracy: 0.8900 - 14s/epoch - 17ms/step\n",
      "Epoch 48/60\n",
      "\n",
      "Epoch 48: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0040 - categorical_accuracy: 0.9995 - val_loss: 0.3524 - val_categorical_accuracy: 0.9200 - 14s/epoch - 17ms/step\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 49: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0301 - categorical_accuracy: 0.9915 - val_loss: 0.4448 - val_categorical_accuracy: 0.9010 - 14s/epoch - 17ms/step\n",
      "Epoch 50/60\n",
      "\n",
      "Epoch 50: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0094 - categorical_accuracy: 0.9973 - val_loss: 0.3853 - val_categorical_accuracy: 0.9110 - 14s/epoch - 17ms/step\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 51: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0260 - categorical_accuracy: 0.9923 - val_loss: 0.4463 - val_categorical_accuracy: 0.9010 - 14s/epoch - 17ms/step\n",
      "Epoch 52/60\n",
      "\n",
      "Epoch 52: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0116 - categorical_accuracy: 0.9965 - val_loss: 0.6947 - val_categorical_accuracy: 0.8570 - 14s/epoch - 17ms/step\n",
      "Epoch 53/60\n",
      "\n",
      "Epoch 53: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0218 - categorical_accuracy: 0.9930 - val_loss: 0.4834 - val_categorical_accuracy: 0.8880 - 14s/epoch - 17ms/step\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 54: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0230 - categorical_accuracy: 0.9930 - val_loss: 0.4228 - val_categorical_accuracy: 0.8900 - 14s/epoch - 17ms/step\n",
      "Epoch 55/60\n",
      "\n",
      "Epoch 55: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0168 - categorical_accuracy: 0.9948 - val_loss: 0.4744 - val_categorical_accuracy: 0.8770 - 14s/epoch - 17ms/step\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 56: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0150 - categorical_accuracy: 0.9950 - val_loss: 1.3099 - val_categorical_accuracy: 0.7560 - 14s/epoch - 17ms/step\n",
      "Epoch 57/60\n",
      "\n",
      "Epoch 57: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0184 - categorical_accuracy: 0.9933 - val_loss: 0.3368 - val_categorical_accuracy: 0.9190 - 14s/epoch - 17ms/step\n",
      "Epoch 58/60\n",
      "\n",
      "Epoch 58: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0115 - categorical_accuracy: 0.9970 - val_loss: 0.4766 - val_categorical_accuracy: 0.8890 - 14s/epoch - 17ms/step\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 59: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0187 - categorical_accuracy: 0.9920 - val_loss: 0.4557 - val_categorical_accuracy: 0.8950 - 14s/epoch - 17ms/step\n",
      "Epoch 60/60\n",
      "\n",
      "Epoch 60: val_categorical_accuracy did not improve from 0.92500\n",
      "800/800 - 14s - loss: 0.0160 - categorical_accuracy: 0.9952 - val_loss: 0.3942 - val_categorical_accuracy: 0.9110 - 14s/epoch - 17ms/step\n",
      "Epoch 60: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_array_from_npy(file_name):\n",
    "    return np.load(file_name)\n",
    "\n",
    "path = './data/SUBESCOxUIU/train/SUBESCO'\n",
    "\n",
    "train_data_x = load_array_from_npy(path + '/npy/train_data_x.npy')\n",
    "train_data_y = load_array_from_npy(path + '/npy/train_data_y.npy')\n",
    "\n",
    "test_data_x = load_array_from_npy('./data/SUBESCOxUIU/test/SUBESCO/npy/test_data_x.npy')\n",
    "test_data_y = load_array_from_npy('./data/SUBESCOxUIU/test/SUBESCO/npy/test_data_y.npy')\n",
    "\n",
    "print(train_data_x.shape)\n",
    "print(train_data_y.shape)\n",
    "print(test_data_x.shape)\n",
    "print(test_data_y.shape)\n",
    "\n",
    "train_data_y = to_categorical(train_data_y)\n",
    "test_data_y = to_categorical(test_data_y)\n",
    "\n",
    "emotion='SUBESCO_latest'\n",
    "\n",
    "acc=train(train_data_x, train_data_y, emotion, 5, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1292, 128, 257, 3)\n",
      "(1292,)\n",
      "SUBESCOxUIU_latest\n",
      "./data/SUBESCOxUIU/train\\model\\SUBESCOxUIU_latest_max_model.keras\n",
      "259/259 [==============================] - 19s 8ms/step - loss: 0.8840 - categorical_accuracy: 0.8529\n",
      "Test accuracy: 0.8529411554336548\n",
      "259/259 [==============================] - 2s 5ms/step\n",
      "Confusion matrix saved to: ./data/SUBESCOxUIU/train\\data\\TR-combo-TST-uiu_confusion.csv\n"
     ]
    }
   ],
   "source": [
    "def test_emotion(test_data_x, test_data_y, total, emotion, path):\n",
    "    model_path = os.path.join(path, 'model', 'SUBESCOxUIU_latest_max_model.keras')\n",
    "    \n",
    "    print(model_path)\n",
    "    \n",
    "    # Check if the file exists before loading\n",
    "    if not os.path.isfile(model_path):\n",
    "        ..................\n",
    "        raise ValueError(f\"File not found: {model_path}. Please ensure the file is an accessible `.keras` zip file.\")\n",
    "    \n",
    "    new_model = load_model(model_path)\n",
    "    \n",
    "    # Ensure the labels are one-hot encoded for testing\n",
    "    test_data_y = to_categorical(test_data_y, num_classes=total)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    history = new_model.evaluate(test_data_x, test_data_y, batch_size=5)\n",
    "    print(f\"Test accuracy: {history[1]}\")\n",
    "    \n",
    "    # Predict and generate confusion matrix\n",
    "    predictions = new_model.predict(test_data_x, batch_size=5)\n",
    "    confusion_matrix = np.zeros((total, total))\n",
    "    \n",
    "    for i in range(len(test_data_y)):\n",
    "        actual_label = np.argmax(test_data_y[i])\n",
    "        predicted_label = np.argmax(predictions[i])\n",
    "        confusion_matrix[actual_label][predicted_label] += 1\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    save_dir = os.path.join(path, 'data')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the confusion matrix to CSV\n",
    "    confusion_csv_path = os.path.join(save_dir, emotion + '_confusion.csv')\n",
    "    with open(confusion_csv_path, \"w\", newline='') as my_csv:\n",
    "        csvWriter = csv.writer(my_csv, delimiter=',')\n",
    "        csvWriter.writerows(confusion_matrix)\n",
    "    \n",
    "    print(f\"Confusion matrix saved to: {confusion_csv_path}\")\n",
    "    \n",
    "\n",
    "path = './data/SUBESCOxUIU/train'\n",
    "\n",
    "# test_data_x = load_array_from_npy(path + '/saved/test_data_x.npy')\n",
    "# test_data_y = load_array_from_npy(path + '/saved/test_data_y.npy')\n",
    "def load_array_from_npy(file_name):\n",
    "    return np.load(file_name)\n",
    "\n",
    "\n",
    "test_data_x = load_array_from_npy('./data/SUBESCOxUIU/test/npy/test_data_x.npy')\n",
    "test_data_y = load_array_from_npy('./data/SUBESCOxUIU/test/npy/test_data_y.npy')\n",
    "\n",
    "print(test_data_x.shape)\n",
    "print(test_data_y.shape)\n",
    "print(emotion)\n",
    "test_emotion(test_data_x, test_data_y, 5, 'TR-combo-TST-uiu', path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
